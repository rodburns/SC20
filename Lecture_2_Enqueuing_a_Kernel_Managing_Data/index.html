<!DOCTYPE html>

<html>
	<head>
	    <meta charset="utf-8">
		<link rel="stylesheet" href="../common-revealjs/css/reveal.css">
		<link rel="stylesheet" href="../common-revealjs/css/theme/white.css">
		<link rel="stylesheet" href="../common-revealjs/css/custom.css">
		<script>
			// This is needed when printing the slides to pdf
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
		<script>
		    // This is used to display the static images on each slide,
			// See global-images in this html file and custom.css
			(function() {
				if(window.addEventListener) {
					window.addEventListener('load', () => {
						let slides = document.getElementsByClassName("slide-background");

						if (slides.length === 0) {
							slides = document.getElementsByClassName("pdf-page")
						}

						// Insert global images on each slide
						for(let i = 0, max = slides.length; i < max; i++) {
							let cln = document.getElementById("global-images").cloneNode(true);
							cln.removeAttribute("id");
							slides[i].appendChild(cln);
						}

						// Remove top level global images
						let elem = document.getElementById("global-images");
						elem.parentElement.removeChild(elem);
					}, false);
				}
			})();
		</script>
		
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<div id="global-images" class="global-images">
					<img src="../common-revealjs/images/sycl_academy.png" />
					<img src="../common-revealjs/images/sycl_logo.png" />
					<img src="../common-revealjs/images/trademarks.png" />
				</div>
				<!--Slide 1-->
				<section class="hbox" data-markdown>
					## SYCL Kernel Functions
				</section>
				<!--Slide 2-->
				<section class="hbox" data-markdown>
					## Learning Objectives

					* Learn about the SYCL execution model
					* Learn about the SIMT model for describing parallelism
					* Learn how to define and invoke SYCL kernel functions
					* Learn about the rules and restrictions on kernel functions
					* Learn how to use both lambdas and function objects
				</section>
				<!--Slide 3-->
				<section class="hbox" data-markdown>
				    ## The SYCL Execution Model
				</section>
				<!--Slide 4-->
				<section>
					<div class="container">
						<div class="col-left" data-markdown>
							* SYCL kernel functions are executed by **work-items**
							* You can think of a work-item as a thread of execution
							* Each work-item will execute a SYCL kernel function from start to end
							* A work-item can run on CPU threads, SIMD lanes, GPU threads, or any other kind of processing element
						</div>
						<div class="col-right" data-markdown>
							![Work Item](../common-revealjs/images/workitem.png "Work-Item")
						</div>

					</div>
				</section>
				<!--Slide 5-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Work-items are collected together into **work-groups**
							* The size of work-groups is generally relative to what is optimal on the device being targeted
							* It can also be affected by the resources used by each work-item
						</div>
						<div class="col" data-markdown>
							![Work Group](../common-revealjs/images/workgroup.png "Work-Group")
						</div>
					</div>
				</section>
				<!--Slide 6-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* SYCL kernel functions are invoked within an **nd-range**
							* An nd-range has a number of work-groups and subsequently a number of work-items
							* Work-groups always have the same number of work-items
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 7-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* The nd-range describes an **iteration space**; how the work-items and work-groups are composed
							* An nd-range can be 1, 2 or 3 dimensions
							* An nd-range has two components
							  * The **global-range** describes the total number of workitems in each dimension
							  * The **local-range** describes the number of work-items in a work-group in each dimension
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-example.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 8-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Each invocation in the iteration space of an nd-range is a work-item
							* Each invocation knows which work-item it is on and can query certain information about its position in the nd-range
							* Each work-item has the following:
							  * **Global range**: {12, 12}
							  * **Global id**: {6, 5}
							  * **Group range**: {3, 3}
							  * **Group id**: {1, 1}
							  * **Local range**: {4, 4}
							  * **Local id**: {2, 1}
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-example-work-item.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 9-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							Typically an nd-range invocation SYCL will execute the SYCL kernel function on a very large number of work-items, often in the thousands
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-invocation.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 10-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Multiple work-items will generally execute concurrently
							* On vector hardware this is often done in lock-step, which means the same hardware instructions
							* The number of work-items that will execute concurrently can vary from one device to another
							* Work-items will be batched along with other work-items in the same work-group
							* The order work-items and workgroups are executed in is implementation defined
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-lock-step.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 11-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Work-items in a work-group can be synchronized using a work-group barrier
							  * All work-items within a work-group must reach the barrier before any can continue on
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/work-group-0.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 12-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* SYCL does not support synchronizing across all work-items in the nd-range
							* The only way to do this is to split the computation into separate SYCL kernel functions
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/work-group-0-1.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 13-->
				<section class="hbox" data-markdown>
					## SYCL is an SIMT Progamming Model
				</section>
				<!--Slide 14-->
				<section>
					<div class="container">
						<div class="col">
							Sequential CPU code
							<code><pre>
void calc(int *in, int *out) {
  // all iterations are run in the same 
  // thread in a loop 
  for (int i = 0; i < 1024; i++){ 
    out[i] = in[i] * in[i];
  } 
}

// calc is invoked just once and all 
// iterations are performed inline 
calc(in, out);
							</code></pre>
						</div>
						<div class="col">
							Parallel SIMT code
							<code><pre>
void calc(int *in, int *out, int id) { 
  // function is described in terms of 
  // a single iteration 
  out[id] = in[id] * in[id];
}

// parallel_for invokes calc multiple 
// times in parallel 
parallel_for(calc, in, out, 1024);


							</code></pre>
						</div>
					</div>
				</section>
				<!--Slide 15-->
				<section class="hbox" data-markdown>
					## Enqueueing SYCL Kernel Functions
				</section>
				<!--Slide 16-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue gpuQueue(gpu_selector{});

  gpuQueue.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(range&lt1&gt(1024), 
      [=](id&lt1&gt i) { 
      // kernel code }); 
  }); 
  gpuQueue.wait();
}

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* SYCL kernel functions are defined and invoked using one of the kernel function invoke APIs provided by the **handler** class
							* These add a SYCL kernel function command to the command group
							* There can only be one SYCL kernel function command in a command group
						</div>
					</div>
				</section>
				<!--Slide 17-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue gpuQueue(gpu_selector{});

  gpuQueue.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(range&lt1&gt(1024), 
      <mark>[=](id&lt1&gt i) { 
      // kernel code }); 
  }); </mark>
  gpuQueue.wait();
}

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* The lambda or function object represents the SYCL device function
							* This is the part that is compiled for the device
						</div>
					</div>
				</section>
				<!--Slide 18-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue gpuQueue(gpu_selector{});

  gpuQueue.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(<mark>range&lt1&gt(1024), 
      [=](id&lt1&gt i</mark>) { 
      // kernel code }); 
  }); 
  gpuQueue.wait();
}
							</code></pre>
						</div>
						<div class="col" data-markdown>
							* There are a number of different APIs for expressing different forms of parallelism, complexity and functionality
							* Each takes some representation of the nd-range and expects a certain parameter type that describes the current index into the iteration space
							* These types have a number of member functions for retrieving different index and range information about the current iteration
						</div>
					</div>
				</section>
				<!--Slide 19-->
				<section>
					Expressing Parallelism
					<div class="container">
						<div class="col">
							<code><pre>
cgh.<mark>single_task</mark><T>([=](){
  // SYCL kernel function is executed 
  // once on a single work-item
});
							</code></pre>
							<code><pre>
cgh.<mark>parallel_for</mark><T>(range&lt2&gt(64, 64), 
                          [=](id&lt2&gt idx){
  // SYCL kernel function is executed 
  // on an nd-range of work-items
});
							</code></pre>
						</div>
						<div class="col">
							<code><pre>
cgh.<mark>parallel_for_work_group</mark>(range&lt2&gt(64, 64), 
                     [=](group&lt2&gt gp){
  // SYCL kernel function is executed  
  // once per work-group
							</code></pre>
							<code><pre>
  <mark>parallel_for_work_item</mark>(gp, [=](h_item&lt2&gt it){
    // SYCL kernel function is executed  
    // once per work-item
  });
});
							</code></pre>
						</div>
					</div>
				</section>
				<!--Slide 20-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(<mark>range&lt1&gt(1024)</mark>, 
    [=](<mark>id&lt1&gt idx</mark>){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(<mark>range&lt1&gt(1024)</mark>, 
    [=](<mark>item&lt1&gt item</mark>){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(nd_range&lt1&gt(<mark>range&lt1&gt(1024), 
    range&lt1&gt(32))</mark>,[=](<mark>nd_item&lt1&gt ndItem</mark>){

  // kernel code

});

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* Overload taking a **range** object specifies the global range, runtime decides local range
							* An **id** parameter represents the index within the global range
							____________________________________________________________________________________________
							* Overload taking a **range** object specifies the global range, runtime decides local range
							* An **item** parameter represents the global range and the index within the global range
							____________________________________________________________________________________________
							* Overload taking an **nd_range** object specifies the global and local range
							* An **nd_item** parameter represents the global and local range and index
						</div>
					</div>
				</section>
				<!--Slide 21-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(range&lt1&gt(1024), 
    [=](<mark>id&lt1&gt(512)</mark>, id&lt1&gtidx){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(range&lt1&gt(1024), 
     <mark>id&lt1&gt(512)</mark>, [=](item&lt1&gt item){
      
  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(nd_range&lt1&gt(range&lt1&gt(1024), 
    range&lt1&gt(32), <mark>id&lt1&gt(512)</mark>),
    [=](nd_item&lt1&gt ndItem){

  // kernel code

});

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* All overloads of **parallel_for** also allow you to optionally specify an offset
							* The offset, if used, will increment each index into the global index by the specified value
							  * E.g. with a range of 1024 and an offset of 512 the indexes would become (512, 1536)
						</div>
					</div>
				</section>
				<!--Slide 22-->
				<section class="hbox" data-markdown>
					## SYCL kernel function rules and restrictions
				</section>
				<!--Slide 23-->
				<section>
					SYCL Kernel Function Rules
					<div class=container data-markdown>
						* Must be defined using a C++ lambda or function object, they cannot be a function pointer or std::function
						* Must always capture or store members by-value
						* SYCL kernel functions declared with a lambda must be named using a forward declarable C++ type, declared in global scope 
						* SYCL kernel function names follow C++ ODR rules, which means you cannot have two kernels with the same name
					</div>
				</section>
				<!--Slide 24-->
				<section>
					SYCL Kernel Function Restrictions
					<div class=container data-markdown>
						* No dynamic allocation
						* No dynamic polymorphism
						* No function pointers
						* No recursion
					</div>
				</section>
				<!--Slide 25-->
				<section class="hbox" data-markdown>
					## SYCL kernels as function objects
				</section>
				<!--Slide 26-->
				<section>
					<div class="container">
						<div class="col-left-3">
							<code><pre>
#include &ltCL/sycl.hpp&gt
using namespace cl::sycl;
class add;

int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue gpuQueue(gpu_selector{}, 
      async_handler{});
    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    gpuQueue.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for&ltadd&gt(range&lt1&gt(dA.size()),
        <mark>[=](id&lt1&gt i){ out[i] = inA[i] + inB[i]; }</mark>);
    }); 
    gpuQueue.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-1" data-markdown>
							* All the examples of SYCL kernel functions up until now have been defined using lambdas
						</div>
					</div>
				</section>
				<!--Slide 27-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
struct add { 
  using read_accessor_t = 
    accessor&ltfloat, 1, 
    access::mode::read, 
    access::target::global_buffer&gt;
  using write_accessor_t = 
    accessor&ltfloat, 1 
    access::mode::write, 
    access::target::global_buffer&gt;
    
  read_accessor_t inA_, inB_; 
  write_accessor_t out_;

void operator()(id&lt1&gt i){ 
  out_[i] = inA_[i] + inB_[i]; 
  }
};
							</code></pre>
						</div>
						<div class="col" data-markdown>
							* As well as defining SYCL kernels using lambdas you can also define a SYCL kernel using a regular C++ function object
							* Where the accessors are stored as members and the function call operator takes the appropriate parameter
						</div>
					</div>
				</section>
				<!--Slide 28-->
				<section>
					<div class="container">
						<div class="col-left-3">
							<code><pre>
#include &ltCL/sycl.hpp&gt
using namespace cl::sycl;
class add;

int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue gpuQueue(gpu_selector{}, 
      async_handler{});
    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    gpuQueue.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      <mark>cgh.parallel_for(range&lt1&gt(dA.size()), 
        add{inA, inB, out});</mark>
    }); 
    gpuQueue.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-1">
						<code><pre>
struct add { 
  using read_accessor_t = 
    accessor&ltfloat, 1, 
    access::mode::read, 
    access::target::global_buffer&gt; 
  using write_accessor_t = 
    accessor&ltfloat, 1 
    access::mode::write, 
    access::target::global_buffer&lt;
  
  read_accessor_t inA_, inB_; 
  write_accessor_t out_;

  void operator()(id&lt1&gt i) ){ 
  out[i] = inA[i] + inB[i]; 
  } 
};
						</code></pre>
						
							To use a C++ function object you simply construct an instance of the type initialising the accessors and pass it to parallel_for<br><br>
							
							Notice you no longer need to name the SYCL kernel
						</div>
					</div>
				</section>
				<!--Slide 1-->
				<section class="hbox" data-markdown>
					## Managing Data
				</section>
				<!--Slide 2-->
				<section class="hbox" data-markdown>
					## Learning Objectives
					* Learn about the buffer/accessor model for managing data
					* Learn how to use buffers and accessors
					* Learn how to access data in a kernel function
					* Learn how a buffer synchronizes data
				</section>
				<!--Slide 3-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container" data-markdown>
						* SYCL separates the storage and access of data 
						  * A SYCL buffer manages data across the host and any number of devices 
						  * A SYCL accessor requests access to data on the host or on a device for a specific SYCL kernel function
						* Accessors are also used to access data within a SYCL kernel function
						  * This means they are declared in the host code but captured by and then accessed within a SYCL kernel function
					</div>
				</section>
				<!--Slide 4-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* A SYCL buffer can be constructed with a pointer to host memory
							* For the lifetime of the buffer this memory is owned by the SYCL runtime
							* When a buffer object is constructed it will not allocate or copy to device memory at first
							* This will only happen once the SYCL runtime knows the data needs to be accessed and where it needs to be accessed
						</div>
						<div class="col" data-markdown>
							![Buffer Host Memory](../common-revealjs/images/buffer-hostmemory.png "Buffer Host Memory")
						</div>
					</div>
				</section>
				<!--Slide 5-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* Constructing an accessor specifies a request to access the data managed by the buffer
							* There are a range of different types of accessor which provide different ways to access data
						</div>
						<div class="col" data-markdown>
							![Buffer Host Memory Accessor](../common-revealjs/images/buffer-hostmemory-accessor.png "Buffer Host Memory Accessor")
						</div>
					</div>
				</section>
				<!--Slide 6-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* When an accessor is constructed it is associated with a command group via the handler object
							* This connects the buffer that is being accessed, the way in which it’s being accessed and the device that the command group is being submitted to
						</div>
						<div class="col" data-markdown>
							![Buffer Host Memory Accessor CG](../common-revealjs/images/buffer-hostmemory-accessor-cg.png "Buffer Host Memory Accessor CG")
						</div>
					</div>
				</section>
				<!--Slide 7-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* Once the SYCL scheduler selects the command group to be executed it must first satisfy its data dependencies
							* This means allocating and copying data to the device the data is being accessed on if necessary
							* If the most recent copy of the data is already on the device then the runtime will not copy again
						</div>
						<div class="col" data-markdown>
							![Buffer Host Memory Accessor CG Device](../common-revealjs/images/buffer-hostmemory-accessor-cg-device.png "Buffer Host Memory Accessor CG Device")
						</div>
					</div>
				</section>
				<!--Slide 8-->
				<section>
					<div class="hbox" data-markdown>
						#### SYCL Buffers & Accessors
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* Data will remain in device memory after kernels finish executing until another command group requests access in a different device or on the host
							* When the buffer object is destroyed it will wait for any outstanding work that is accessing the data to complete and then copy back to the original host memory
						</div>
						<div class="col" data-markdown>
							![Buffer Destroyed](../common-revealjs/images/buffer-destroyed.png "Buffer Destroyed")
						</div>
					</div>
				</section>
				<!--Slide 9-->
				<section>
					<div class="hbox" data-markdown>
						#### Buffer class
					</div>
					<div class="container">
						<code class="code-100pc"><pre>
template &lt;typename dataT, int dimensions&gt;
buffer;
						</code></pre>
					</div>
					<div class="container" data-markdown>
						* A `buffer` manages data across the host application
						and kernel functions executing on device(s).
						* It has a typename which specifies the type of the
						elements of data it manages.
						* It has a dimensionality which specifies the
						dimensionality that the elements of data are represented
						in.
					</div>
				</section>
				<!--Slide 10-->
				<section>
					<div class="hbox" data-markdown>
						#### Constructing a buffer
					</div>
					<div class="container">
						<code class="code-100pc"><pre>
auto buf = sycl::buffer{&var, sycl::range{1}};
						</code></pre>
					</div>
					<div class="container" data-markdown>
						* A `buffer` can be constructed from a pointer to data
						for it to manage and a `range` which describes the
						number of elements of data.
						* Using CTAD the type and the dimensionality can be
						inferred.
					</div>
				</section>
				<!--Slide 11-->
				<section>
					<div class="hbox" data-markdown>
						#### Accessor class
					</div>
					<div class="hbox" data-markdown>
						![Accessor Types](../common-revealjs/images/accessor-types.png "Accessor Types")
					</div>
				</section>
				<!--Slide 12-->
				<section>
					<div class="hbox" data-markdown>
						#### Accessor class
					</div>
					<div class="container" data-markdown>
						* There are many different ways to use the `accessor`
						class.
						  * Accessing data on a device.
						  * Accessing data immediately in the host application.
						  * Allocating local memory.
						* For now we are going to focus on accessing data on a
						device.
					</div>
				</section>
				<!--Slide 13-->
				<section>
					<div class="hbox" data-markdown>
						#### Constructing an accessor
					</div>
					<div class="container">
						<code class="code-100pc"><pre>
auto acc = accessor&lt;float, 1, access::mode::read_write, access::target::global_buffer&gt;(buf, cgh);
						</code></pre>
					</div>
					<div class="container" data-markdown>
						* When creating an `accessor` you have to specify the various template parameters.
						  * The element type should be the same as the `buffer`.
						  * The dimensionality should be the same as the
						  `buffer`.
						  * The `access::mode` will specify how you wish to
						  access the data.
						  * The `access::target` should be `global_buffer`.
						* When constructing an `accessor` you have to provide
						  the `buffer` being accessed and the `handler` associated with he command group you are within.
					</div>
				</section>
				<!--Slide 14-->
				<section>
					<div class="hbox" data-markdown>
						#### get_access
					</div>
					<div class="container">
						<code class="code-100pc"><pre>
auto acc = buf.get_access&lt;access::mode::read_write&gt;(cgh);
						</code></pre>
					</div>
					<div class="container" data-markdown>
						* Alternatively the `buffer` class provides the
						`get_access` member function as a shortcut.
						  * This infers the element type and dimensionality
						  from the `buffer` and assumes the `access::target` is
						  `global_buffer`.
						* You just need to specify the `access::mode` and
						provide the `handler` as before.
					</div>
				</section>
				<!--Slide 15-->
				<section>
					<div class="hbox" data-markdown>
						#### Access modes
					</div>
					<div class="container">
						<div class="col" data-markdown>
							* A **read** `accessor` instructs the SYCL runtime that the SYCL kernel function will read the data – cannot be written to within a SYCL kernel function.
							* A **write** `accessor` instructs the SYCL runtime that the SYCL kernel function will modify the data – creating a dependency for future command groups.
							* A **discard_*** `accessor` instructs the SYCL runtime that the SYCL kernel function does not need the initial values of the data – removing the dependency on previous command groups.
						</div>
						<div class="col" data-markdown>
							![Buffer Accessors](../common-revealjs/images/accessors.png "Buffer Accessors")
						</div>
					</div>
				</section>
				<!--Slide 16-->
				<section>
					<div class="hbox" data-markdown>
						#### Access resolution
					</div>
					<div class="hbox" data-markdown>
						* If a command group has accessors to the same buffer with conflicting access modes they are resolved into one:
						  * read & write => ```read_write```.
						  * ```read_write``` & ```discard_write``` => ```read_write```.
						* Within the SYCL kernel function there are still multiple accessors, but they alias to the same memory address.
					</div>
				</section>
				<!--Slide 17-->
				<section>
					<div class="hbox" data-markdown>
						#### Access resolution
					</div>
					<div class="container">
						<div class="col-left-3">
							<code><pre>
gpuQueue.submit([&](handler &cgh){
  auto in = bufA.get_access&lt;<mark>access::mode::read</mark>&gt;(cgh);
  auto out = bufO.get_access&lt;<mark>access::mode::write</mark>&gt;(cgh);
});
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* Here **in** and **out** both point to bufI but one is access::mode::read and one is access::mode::write.
							* So the SYCL runtime will treat them both as `access::mode::read_write`.
							* Both will point to a single allocation of global memory on the device(s).
							* The runtime will resolve the data dependency into `access::mode::read_write`.
						</div>
					</div>
				</section>
				<!--Slide 18-->
				<section>
					<div class="hbox" data-markdown>
						#### operator[]
					</div>
					<div class="container">
						<code class="code-100pc"><pre>
gpuQueue.submit([&](handler &cgh){
  auto inA = buf.get_access&ltaccess::mode::read&gt(cgh);
  auto inB = buf.get_access&ltaccess::mode::read&gt(cgh);
  auto inO = buf.get_access&ltaccess::mode::write&gt(cgh);
  cgh.single_task&lt;add&gt;([=](){
    <mark>out[0] = inA[0] + inB[0];</mark>
  }); 
});
						</code></pre>
					</div>
					<div class="container" data-markdown>
						* As well as specifying data dependencies an `accessor`
						can also be used to access the data from within a kernel
						function.
						* You can do this by calling `operator[]` on the
						`accessor`.
						  * This operator can take an `id` or a `size_t`.
					</div>
				</section>
			</div>
		</div>
		<script src="../common-revealjs/js/reveal.js"></script>
		<script src="../common-revealjs/plugin/markdown/marked.js"></script>
		<script src="../common-revealjs/plugin/markdown/markdown.js"></script>
		<script src="../common-revealjs/plugin/notes/notes.js"></script>
		<script>
			Reveal.initialize({mouseWheel: true, defaultNotes: true});
			Reveal.configure({ slideNumber: true });
		</script>
	</body>
</html>
